# -*- coding: utf-8 -*-
"""Assignment01_EDA_JoshuaJex.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZDTGRlHCUrqRiuix5s74GPL6v-k6xBOW

# DS 2500 Exploratory Data Analysis
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sb

import numpy as np
import pandas as pd
import scipy.stats as stats
from scipy.stats import chi2_contingency



"""# 1. Formulate your questions

Question: Is there a dependency between:

1.   Car insurance premiums and fatal collisions by state?
2.   Car insurance premiums and insurance losses incurred by car wrecks by state?

# 2. Read in your data

From DS2500 Git Hub Repo, ["Highway Safety Data"](https://github.com/harsh1399/DS2500-Data_Wrangling/blob/main/Module_Assignment-1/Highway_Safety_Data/bad-drivers.csv)
"""

#Reads in a data frame from the Highway Safety Data git hub csv.
df = pd.read_csv('https://raw.githubusercontent.com/harsh1399/DS2500-Data_Wrangling/main/Module_Assignment-1/Highway_Safety_Data/bad-drivers.csv')

"""# 3. Check the packaging

A 51 row by 8 column shape is what we expect.
"""

df.shape

"""# 4. Look at the top and bottom of data

Comparing the head and tail of our data to the raw csv, we can conclude that our data was read and processed properly.
"""

df.head()

df.tail()

"""# 5. Check the "n"s

There are 51 states which is correct (District of Columbia is included).
"""

df.State.count()

"""# 6. Validate against an external knowledge or data source

The lowest and highest percentage of alchohol impaired drivers by state.
"""

alcoholDriversColumnMinIndex = df['Percentage Of Drivers Involved In Fatal Collisions Who Were Alcohol-Impaired'].idxmin()
df['State'].get(alcoholDriversColumnMinIndex)

"""The state with the lowest 'Percentage Of Drivers Involved In Fatal Collisions Who Were Alcohol-Impaired' is Montana. According to "[statisa](https://www.statista.com/statistics/442848/per-capita-alcohol-consumption-of-all-beverages-in-the-us-by-state/)", Utah drinks around 1.23 gallons of ethanol per person each year, ranking Utah 51st overall for the states (Note: District of Columbia was included in the statisa ranking). Therefore, having Utah has the lowest in this category is plausible accoridng to outside sources."""

alcoholDriversColumnMaxIndex = df['Percentage Of Drivers Involved In Fatal Collisions Who Were Alcohol-Impaired'].idxmax()
df['State'].get(alcoholDriversColumnMaxIndex)

"""The state with the highest 'Percentage Of Drivers Involved In Fatal Collisions Who Were Alcohol-Impaired' is Montana. According to "[statisa](https://www.statista.com/statistics/442848/per-capita-alcohol-consumption-of-all-beverages-in-the-us-by-state/)", Montana drinks around 3.21 gallons of ethanol per person each year, ranking Montana 6th overall for the states. Therefore, having Montana has the highest in this category is plausible accoridng to outside sources.

# 7. Visualization


Number of drivers involved in fatal collisions per billion miles and Car Insurance Premiums.
"""

df.plot.scatter(x = 'Number of drivers involved in fatal collisions per billion miles', y = 'Car Insurance Premiums ($)', title = 'Collisions vs Insurance Premiums')

df.plot.scatter(x = 'Losses incurred by insurance companies for collisions per insured driver ($)', y = 'Car Insurance Premiums ($)', title = 'Collisions vs Insurance Premiums')

"""# 8. Statisical Test

**Chi square test for fatalities in car accidents vs insurance premiums.**

Using a null hypothesis that the number of drivers involved in fatal collsions per billion miles by state and car insurance premiums by state are dependent with a significant level of 0.05, we can reject the null hypothesis and conclude that the number of drivers in fatal collisions and car insurance premiums are dependent since p < 0.05
"""

FatalitiesVsPremiumsTable = (df['Number of drivers involved in fatal collisions per billion miles'], df['Car Insurance Premiums ($)'])
stat, p, dof, expected = chi2_contingency(FatalitiesVsPremiumsTable)
print('P value: ' + format(p))

"""**Chi square test for losses to insurance companies vs insurance premiums.**

Using a null hypothesis that losses incurred by insurance companies for collisions per insured driver and car insurance premiums with a significance level of 0.05, we can reject the null hypothesis and conclude that the number of losses from insurance companies and insurance premiums are dependent since p < 0.05.
"""

LossesFromInsuranceVsPremiumsTable = (df['Losses incurred by insurance companies for collisions per insured driver ($)'], df['Car Insurance Premiums ($)'])
stat, p, dof, expected = chi2_contingency(LossesFromInsuranceVsPremiumsTable)
print('P value: ' + format(p))

"""Write up:

The question for this exploratory data analysis was: Is there a dependency between:

1.   **Car insurance premiums and fatal collisions by state?**
2.   **Car insurance premiums and insurance losses incurred by car wrecks by state?**

**Why this question:** I asked this question to see if there is a relationship between insurance and fatal car wrecks as well as to explore a relationship between insurance company pay outs and premiums. With the high price of car insurance, it’s interesting to question how predictive these premiums are for car accidents. Thus the purpose of this experiment is to explore the dependence of these relationships.

**Visualization of data:** For my visualization I chose a scatter plot to explore the relationship between these variables across states. Since we’re not tracking these variables across time, a visualization such a line graph wouldn’t have worked as well. Instead, a scatter plot allows us to analyze a general relationship between insurance and fatal accidents/insurance losses and see if there is at least a broad trend between them. For insurance premiums versus fatal collisions, the scatter plot was not as effective in visualizing a relationship between the two columns. The plot makes it seem as if there is no relationship between the two, even though a statistical test shows this is not true. However, the scatter plot was quite effective at determining a relationship between insurance premiums and losses incurred by car wrecks as there seems to be some type of linear relationship between the two.

**Statistical Methods:** The statistical method for this question and analysis was a chi squared test. A chi squared test is used to determine the dependence between two variables which matches our question of determining the dependence of premiums and fatal accidents/insurance losses. Thus two chi squared tests were used in this analysis:

*   **Car insurance premiums and fatal collisions chi squared test:** For this test we used a significance value of 0.05. Our null hypothesis is that there is no dependence between these two columns. Our chi squared test resulted in a p value of 0.0002 which is less than 0.05 which implies that we can reject our hypothesis meaning that there is a dependence between fatal collisions and premiums.
*   **Car insurance premiums and insurance losses incurred by car wrecks chi squared test:** For this test we again used a significance value of 0.05. Our null hypothesis is that there is no dependence between these two columns. The chi squared value for this test was 7e-13 which implies that we can reject our hypothesis meaning that there is a dependence between fatal collisions and insurance premiums.


**Data ethics canvas use for this exploratory data analysis:** the data ethics canvas was very useful for this assignment. Some of the modules were not as applicable as “Sharing Data with Others,” since the exploratory data analysis that we do in this assignment will likely never be published. However, many of the modules were useful. For example, considering the limitations of the “Highway Safety Data,” because data is by state and not overtime, proving or even implying that our conclusion stands over time is not possible. Reasoning the collection of the data allowed me to come up with a clear question that could be answered and that is interesting. Since many states require liability insurance for cars, our data and testing has the potential to be engaging with audiences. And lastly, “ongoing implementation” was helpful as I performed tests and made my question more clear in order to make a better exploratory data analysis.


**Data ethics canvas use for other data driven-problems:**

The data science pipeline often seems simple. Find it, clean it, test it, publish it. Many times as data scientists we forget how influential and impactful this pipeline can be, before, during, and after the process is over. We forget that the data is coming from places and people who have a stake in what we’re processing. We forget that there are consequences to how we carry and test the data we receive. That’s why the data science ethics canvas is so important. The data ethics canvas forces us to think about all the ways our data is impacting others during all stages of the data processing pipeline. Considering the sources our data is coming from and who we share the data with makes us treat our data more carefully, especially if that data carries vital personal information. At the same time, being open and transparent about the use and placing of our data helps the data providers understand our plans and our intentions. Take, for example, cookies. Nearly every website now asks permission to track cookies and many provide little or vague information on how your data will be used. I, personally, almost always say no if I can. I don’t know to whom or where my data will go. I don’t know how that data will be used. Even though the companies taking and selling my data might not have bad intentions, a lack of transparency makes me feel like a product target and uncomfortable. I don’t feel like these companies are trying to minimize their impact. In this way, the data ethics canvas acts as a guide to help data processors understand the potential discomfort and misunderstanding that the data science pipeline can produce.

The data ethics canvas also tries to make data scientists not only acknowledge the damage data can do, but take steps to prevent it. Acknowledging data as a potential ethical difficulty makes us naturally want to prevent any ethical disaster from occurring. Thus, we’ll be obliged to take steps such as data protection, non disclosure agreements, and careful data review to ensure that our practices as data wranglers are fair and ethical. The canvas encourages us to take steps to minimize the bad impact of our data as well as to take steps to ensure our practices are legal and in good context.

By considering our actions throughout the data collection process, we can ensure that our analysis can be fair, helpful, and in good faith, providing data that can help, not deter, the world in becoming a better place.

"""
